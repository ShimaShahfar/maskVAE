{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not \"cdSet\" in globals():\n",
    "    %cd -q ..\n",
    "    cdSet = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import models.networks as networks\n",
    "# from collections import OrderedDict\n",
    "from data.vae import Cityscapes\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_beta = 0.0001\n",
    "num_epochs = 10\n",
    "latent_dims = 1024\n",
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "H, W = 512, 512\n",
    "\n",
    "mapping = {\n",
    "    (0, 0, 0): 0,\n",
    "    (0, 0, 0): 1,\n",
    "    (0, 0, 0): 2,\n",
    "    (0, 0, 0): 3,\n",
    "    (0, 0, 0): 4,\n",
    "    (111, 74, 0): 5,\n",
    "    (81, 0, 81): 6,\n",
    "    (128, 64, 128): 7,\n",
    "    (244, 35, 232): 8,\n",
    "    (250, 170, 160): 9,\n",
    "    (230, 150, 140): 10,\n",
    "    (70, 70, 70): 11,\n",
    "    (102, 102, 156): 12,\n",
    "    (190, 153, 153): 13,\n",
    "    (180, 165, 180): 14,\n",
    "    (150, 100, 100): 15,\n",
    "    (150, 120, 90): 16,\n",
    "    (153, 153, 153): 17,\n",
    "    (153, 153, 153): 18,\n",
    "    (250, 170, 30): 19,\n",
    "    (220, 220, 0): 20,\n",
    "    (107, 142, 35): 21,\n",
    "    (152, 251, 152): 22,\n",
    "    (70, 130, 180): 23,\n",
    "    (220, 20, 60): 24,\n",
    "    (255, 0, 0): 25,\n",
    "    (0, 0, 142): 26,\n",
    "    (0, 0, 70): 27,\n",
    "    (0, 60, 100): 28,\n",
    "    (0, 0, 90): 29,\n",
    "    (0, 0, 110): 30,\n",
    "    (0, 80, 100): 31,\n",
    "    (0, 0, 230): 32,\n",
    "    (119, 11, 32): 33,\n",
    "    (0, 0, 142): 34,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_avg = []\n",
    "val_loss_avg = []\n",
    "n_iter = 0\n",
    "num_batches = 0\n",
    "train_loss_avg.append(0)\n",
    "val_loss_avg.append(0)\n",
    "pixel_size = batch_size * H * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(logits, target_onehot, mu, logvar, cur_epoch, name=\"train\"):\n",
    "    pred = torch.argmax(logits, dim=1).float()\n",
    "    target = torch.argmax(target_onehot, dim=1)\n",
    "    recon_loss = F.cross_entropy(logits, target)\n",
    "\n",
    "    acc = (pred == target).float().mean()\n",
    "\n",
    "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    print(\n",
    "        (\n",
    "            f\"{name}\\n\"\n",
    "            f\"epoch #{cur_epoch}\\n\"\n",
    "            f\"reconstruction loss: {recon_loss}\\n\"\n",
    "            f\"kldivergence:  {kldivergence}\\n\"\n",
    "            f\"variational_beta * kldivergence: {variational_beta * kldivergence}\\n\"\n",
    "            f\"batch accuracy {acc}%\\n\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return recon_loss, variational_beta * kldivergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "netVAE = networks.define_VAE(input_nc=34)\n",
    "netVAE = netVAE.to(device)\n",
    "\n",
    "dataset_train = Cityscapes(\n",
    "    root=\"../../maskgan/data/cityscapes/\",\n",
    "    split=\"train\",\n",
    "    mode=\"fine\",\n",
    "    target_type=\"semantic\",\n",
    "    transform=None,\n",
    "    target_transform=None,\n",
    "    transforms=None,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_val = Cityscapes(\n",
    "    root=\"../../maskgan/data/cityscapes/\",\n",
    "    split=\"val\",\n",
    "    mode=\"fine\",\n",
    "    target_type=\"semantic\",\n",
    "    transform=None,\n",
    "    target_transform=None,\n",
    "    transforms=None,\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=netVAE.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "PATH = \"checkpoint_vae/000100.pt\"\n",
    "checkpoint = torch.load(PATH)\n",
    "netVAE.load_state_dict(checkpoint['vae'])\n",
    "\n",
    "# optimizer.load_state_dict(checkpoint['vae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test- epoch #10- reconstruction loss: 0.33180510997772217- kldivergence:  5674.7158203125- variational_beta * kldivergence: 0.5674715638160706- batch accuracy 1889172.0%\n",
      "test- epoch #10- reconstruction loss: 0.4205310344696045- kldivergence:  6018.814453125- variational_beta * kldivergence: 0.6018814444541931- batch accuracy 1840696.0%\n",
      "test- epoch #10- reconstruction loss: 0.3792414367198944- kldivergence:  6110.1162109375- variational_beta * kldivergence: 0.6110116243362427- batch accuracy 1856825.0%\n",
      "test- epoch #10- reconstruction loss: 0.40148717164993286- kldivergence:  6035.4169921875- variational_beta * kldivergence: 0.6035416722297668- batch accuracy 1851292.0%\n",
      "test- epoch #10- reconstruction loss: 0.3707336485385895- kldivergence:  5890.1279296875- variational_beta * kldivergence: 0.5890128016471863- batch accuracy 1862884.0%\n",
      "test- epoch #10- reconstruction loss: 0.3114130198955536- kldivergence:  5834.8056640625- variational_beta * kldivergence: 0.5834805369377136- batch accuracy 1902028.0%\n",
      "test- epoch #10- reconstruction loss: 0.38280320167541504- kldivergence:  6162.7734375- variational_beta * kldivergence: 0.6162773370742798- batch accuracy 1855569.0%\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(dataloader_val):\n",
    "    batch = batch.to(device)\n",
    "    batch = F.one_hot(batch.long(), num_classes=34)\n",
    "    batch = batch.permute(0, 3, 1, 2).float().contiguous()\n",
    "\n",
    "    logits, target_onehot, latent_mu, latent_logvar = netVAE(batch)\n",
    "    losses = vae_loss(\n",
    "        logits, target_onehot, latent_mu, latent_logvar, name=\"test\", cur_epoch=10\n",
    "    )\n",
    "    if idx > 5:\n",
    "        break\n",
    "    # backpropagation\n",
    "#     optimizer.zero_grad()\n",
    "#     recon_loss.backward()\n",
    "#     # one step of the optmizer (using the gradients from backpropagation)\n",
    "#     optimizer.step()\n",
    "#     n_iter += 1\n",
    "#     train_loss_avg[-1] += recon_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for image_batch in dataloader:\n",
    "        netVAE.train()\n",
    "\n",
    "        image_batch = image_batch.to(device)\n",
    "        image_batch = image_batch.view(image_batch.shape[0], image_batch.shape[2], image_batch.shape[3])\n",
    "        mask = F.one_hot(image_batch.type(torch.long),num_classes= 34)\n",
    "        mask = mask.permute(0, 3, 1, 2).float().contiguous()\n",
    "        # vae reconstruction\n",
    "        image_batch_recon, x, latent_mu, latent_logvar = netVAE(mask)\n",
    "        recon_loss = vae_loss(image_batch_recon, mask, latent_mu, latent_logvar, name = 'train', cur_epoch=epoch)\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        recon_loss.backward()\n",
    "        # one step of the optmizer (using the gradients from backpropagation)\n",
    "        optimizer.step()\n",
    "        num_batches += 1\n",
    "        train_loss_avg[-1] += recon_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_batch_recon.type(torch.uint8)\n",
    "pred = torch.argmax(image, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_mapping = {mapping[k]: k for k in mapping}\n",
    "image = mask.type(torch.uint8)\n",
    "pred = torch.argmax(image, dim=1) # or e.g. pred = torch.randint(0, 19, (224, 224))\n",
    "pred_image_1 = torch.zeros(3, pred.shape[1], pred.shape[2], dtype=torch.uint8)\n",
    "for k in rev_mapping:\n",
    "    pred_image_1[:, pred[4]==k] = torch.tensor(rev_mapping[k]).byte().view(3, 1)\n",
    "plt.imshow(pred_image_1.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_mapping = {mapping[k]: k for k in mapping}\n",
    "image = image_batch_recon.type(torch.uint8)\n",
    "pred = torch.argmax(image, dim=1) # or e.g. pred = torch.randint(0, 19, (224, 224))\n",
    "pred_image = torch.zeros(3, pred.shape[1], pred.shape[2], dtype=torch.uint8)\n",
    "for k in rev_mapping:\n",
    "    pred_image[:, pred[4]==k] = torch.tensor(rev_mapping[k]).byte().view(3, 1)\n",
    "plt.imshow(pred_image.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_batch = torch.randint(-30,40,(8,34,512,512), device=device).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch_recon.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss = vae_loss(random_batch, mask, latent_mu, latent_logvar, name = 'test', cur_epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
